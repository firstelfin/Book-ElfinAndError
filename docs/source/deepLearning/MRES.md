# MRES

论文核心与SAM类似，主要的工作是将实例级别的RES任务统一为实例级别+实例组成部分级别。作者认为文图对齐的核心是让模型理解实例结构表示、部分表示...

## 1、数据工程

### 1.1 Multi-Grained Dense Captioner
多粒度稠密提示生成器（LVLM：视觉-文本大模型）。

<img src="https://img2024.cnblogs.com/blog/1319275/202404/1319275-20240416184034167-351606641.png">

[资源链接](https://img2024.cnblogs.com/blog/1319275/202404/1319275-20240416184034167-351606641.png)

如(a)所示，需要微调一个LVLM模型，用于生成图片的：图片级别描述、实例级别描述、实例组件级别描述。训练数据集全部来源于人工精确标注。

**标注**：

1. 图片级别标注：坐标全部放缩到[0, 999], 即全图的坐标为(0,0),(999,999), 使用的COCO数据集；
2. 实例级别：使用的Visual Genome dataset；
3. 对于组件粒度：我们利用单峰语义分割数据 [4,12,33] 并采用 目标Y 的 组件X 形式模板来构造密集的captions。

这种统一的多任务训练方法可以在不同粒度上发挥协同作用：它允许 LVLM 合并更全面、更详细的信息，以增强组件粒度的描述。 同时，组件粒度的知识有助于 LVLM 概括对象内部的知识。

---

### 1.2 实例级别目标GT生成
如(b)所示，我们从Object365获取高质量的标注框，将框作为视觉提示，输入稠密的提示生成器生成实例提示，输入提示分割器(SAM)生成mask.

### 1.3 实例组件目标GT生成
如(c)所示，根据标签与额外的知识(GPT4)生成组件词典，输入开放词汇分割得到bbox和mask, 将bbox输入密集提示生成器生成组件级别的提示。

### 1.4 过滤

完成所有图片的多粒度标注，然后我们利用CLIP进行过滤。使用图片编码与caption编码求相似度，对相似度大于0.5的标注对进行保留。
